<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>速读论文Fast-Bev | 个人博客</title>
<meta name=keywords content="papper"><meta name=description content="摘要：速读论文Fast-BEV: Towards Real-time On-vehicle Bird&rsquo;s-Eye View Perception。"><meta name=author content="Yangly0"><link rel=canonical href=https://yangly0.github.io/posts/20250205211553/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://yangly0.github.io/images/favicon/favicon.png><link rel=icon type=image/png sizes=16x16 href=https://yangly0.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yangly0.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://yangly0.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://yangly0.github.io/images/favicon/favicon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yangly0.github.io/posts/20250205211553/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css integrity=sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js integrity=sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://yangly0.github.io/posts/20250205211553/"><meta property="og:site_name" content="个人博客"><meta property="og:title" content="速读论文Fast-Bev"><meta property="og:description" content="摘要：速读论文Fast-BEV: Towards Real-time On-vehicle Bird’s-Eye View Perception。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-05T21:15:53+00:00"><meta property="article:modified_time" content="2025-02-05T21:15:53+00:00"><meta property="article:tag" content="Papper"><meta property="og:image" content="https://yangly0.github.io/images/favicon/favicon.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://yangly0.github.io/images/favicon/favicon.png"><meta name=twitter:title content="速读论文Fast-Bev"><meta name=twitter:description content="摘要：速读论文Fast-BEV: Towards Real-time On-vehicle Bird&rsquo;s-Eye View Perception。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yangly0.github.io/posts/"},{"@type":"ListItem","position":2,"name":"速读论文Fast-Bev","item":"https://yangly0.github.io/posts/20250205211553/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"速读论文Fast-Bev","name":"速读论文Fast-Bev","description":"摘要：速读论文Fast-BEV: Towards Real-time On-vehicle Bird\u0026rsquo;s-Eye View Perception。\n","keywords":["papper"],"articleBody":"摘要：速读论文Fast-BEV: Towards Real-time On-vehicle Bird’s-Eye View Perception。\n日期：19 Jan 2023 作者：Bin Huang et al. 来源：NeurIPS2022 链接：https://arxiv.org/abs/2301.07870 代码：https://github.com/Sense-GVT/Fast-BEV 思考 问题: BEV感知可以解决激光雷达的成本问题，但是当前的BEV感知受限于\n方法: 文章提出了什么方法和技术；\n结论: 文章结论即 数据集 + 评价 指标；\n优化：还有什么值得改进与优化的。\n理解 论文读完的感受与体会，比如该方法借鉴了什么思想，方法是不是新颖，实验怎么做的，讨论的变量是什么，还有其他值得读的文献。\n如何把2D图像特征转换到3D空间？\n1、BEVFormer类的算法思想采用Transformer的注意力机制基于查询的方法去获取三维BEV特征。缺陷是注意力机制的运算操作对某些计算平台可能不太友好。\n$$ F_{b e v}(x, y, z)=\\operatorname{Attn}(q, k, v) $$\n注意力机制的运算操作 不友好点？\n1、计算复杂度高，O(N^2), N表示序列长度，稀疏性Attention可降低到O(LlogL), 甚至O(L)\n2、存储空间过大\n2、BEVDet类的算法思想通过计算二维特征与预测深度的外积来获得三维BEV特征。缺陷是仅适用于cuda推理库等加速平台，非推理库加速平台部署困难，而且更大的分辨率和特征维度则会遇到计算性能瓶颈。\n$$ F_{b e v}(x, y, z)=\\operatorname{Pool}\\left{F_{2 D}(u, v) \\otimes D(u, v)\\right}_{x, y, z} $$\n考虑到之前的视图变换方法都比较耗时，作者基于M2BEV中提出的沿相机光线方向深度是均匀分布的假设，提出了Fast-Ray视图变换方法，这种方法将多视图的2D图像特征沿相机光线投射到3D体素空间中。该方法的优点是，只要知道相机的内外参，就可以很容易地计算出2D到3D的投影关系。由于这个过程中没有使用可学习的参数，因此可以很容易地计算出图像特征点和BEV特征点之间的对应关系矩阵。\n沿相机光线方向深度是均匀分布假设：//\n1、\nhttps://zhuanlan.zhihu.com/p/709461901\n贡献：\n（1）一种针对图像和 BEV 空间的强大数据增强策略，以避免过度拟合。\n图像上，随机翻转、裁剪和旋转，核心是改变相机内参 BEV上，随机旋转，核心是改变相机外参 （2）一种多帧特征融合机制，以利用时间信息。 三帧历史关键帧，间隔0.5s 投影到相应的 BEV 空间，然后使用相机外部和全局坐标与当前帧对齐 在通道维度上直接连接这些多帧 BEV 特征 在训练阶段，使用图像编码器在线提取历史帧特征。在测试阶段，历史帧功能可以离线保存，直接取出进行加速 （3）一种优化的部署友好型视图转换，以加快推理速度。 预计算投影索引 密集体素 实现可参考，link ","wordCount":"86","inLanguage":"en","image":"https://yangly0.github.io/images/favicon/favicon.png","datePublished":"2025-02-05T21:15:53Z","dateModified":"2025-02-05T21:15:53Z","author":{"@type":"Person","name":"Yangly0"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yangly0.github.io/posts/20250205211553/"},"publisher":{"@type":"Organization","name":"个人博客","logo":{"@type":"ImageObject","url":"https://yangly0.github.io/images/favicon/favicon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yangly0.github.io/ accesskey=h title="个人博客 (Alt + H)">个人博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yangly0.github.io/posts/ title=文章><span>文章</span></a></li><li><a href=https://yangly0.github.io/about/ title=关于><span>关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://yangly0.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://yangly0.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">速读论文Fast-Bev</h1><div class=post-meta><span title='2025-02-05 21:15:53 +0000 UTC'>February 5, 2025</span>&nbsp;·&nbsp;Yangly0</div></header><div class=post-content><p>摘要：速读论文Fast-BEV: Towards Real-time On-vehicle Bird&rsquo;s-Eye View Perception。</p><pre tabindex=0><code>日期：19 Jan 2023
作者：Bin Huang et al.
来源：NeurIPS2022
链接：https://arxiv.org/abs/2301.07870
代码：https://github.com/Sense-GVT/Fast-BEV
</code></pre><h2 id=思考>思考<a hidden class=anchor aria-hidden=true href=#思考>#</a></h2><p>问题: BEV感知可以解决激光雷达的成本问题，但是当前的BEV感知受限于</p><p>方法: 文章提出了什么方法和技术；</p><p>结论: 文章结论即 数据集 + 评价 指标；</p><p>优化：还有什么值得改进与优化的。</p><h2 id=理解>理解<a hidden class=anchor aria-hidden=true href=#理解>#</a></h2><p>论文读完的感受与体会，比如该方法借鉴了什么思想，方法是不是新颖，实验怎么做的，讨论的变量是什么，还有其他值得读的文献。</p><p>如何把2D图像特征转换到3D空间？</p><p>1、<code>BEVFormer</code>类的算法思想采用<code>Transformer</code>的注意力机制基于查询的方法去获取三维<code>BEV</code>特征。缺陷是注意力机制的运算操作对某些计算平台可能不太友好。</p><p>$$
F_{b e v}(x, y, z)=\operatorname{Attn}(q, k, v)
$$</p><blockquote><p>注意力机制的运算操作 不友好点？</p><p>1、计算复杂度高，O(N^2), N表示序列长度，稀疏性Attention可降低到O(LlogL), 甚至O(L)</p><p>2、存储空间过大</p></blockquote><p>2、<code>BEVDet</code>类的算法思想通过计算二维特征与预测深度的外积来获得三维<code>BEV</code>特征。缺陷是仅适用于cuda推理库等加速平台，非推理库加速平台部署困难，而且更大的分辨率和特征维度则会遇到计算性能瓶颈。</p><p>$$
F_{b e v}(x, y, z)=\operatorname{Pool}\left{F_{2 D}(u, v) \otimes D(u, v)\right}_{x, y, z}
$$</p><p>考虑到之前的视图变换方法都比较耗时，作者基于<code>M2BEV</code>中提出的沿相机光线方向深度是均匀分布的假设，提出了<code>Fast-Ray</code>视图变换方法，这种方法将多视图的<code>2D</code>图像特征沿相机光线投射到<code>3D</code>体素空间中。该方法的优点是，只要知道相机的内外参，就可以很容易地计算出<code>2D</code>到<code>3D</code>的投影关系。由于这个过程中没有使用可学习的参数，因此可以很容易地计算出图像特征点和<code>BEV</code>特征点之间的对应关系矩阵。</p><blockquote><p>沿相机光线方向深度是均匀分布假设：//</p><p>1、</p></blockquote><p><a href=https://zhuanlan.zhihu.com/p/709461901>https://zhuanlan.zhihu.com/p/709461901</a></p><p>贡献：</p><p>（1）一种针对图像和 BEV 空间的强大数据增强策略，以避免过度拟合。</p><ul><li>图像上，随机翻转、裁剪和旋转，核心是改变相机内参</li><li>BEV上，随机旋转，核心是改变相机外参
（2）一种多帧特征融合机制，以利用时间信息。</li><li>三帧历史关键帧，间隔0.5s</li><li>投影到相应的 BEV 空间，然后使用相机外部和全局坐标与当前帧对齐</li><li>在通道维度上直接连接这些多帧 BEV 特征</li><li>在训练阶段，使用图像编码器在线提取历史帧特征。在测试阶段，历史帧功能可以离线保存，直接取出进行加速
（3）一种优化的部署友好型视图转换，以加快推理速度。</li><li>预计算投影索引</li><li>密集体素</li><li>实现可参考，<a href=https://blog.csdn.net/qq_41204464/article/details/137571716>link</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://yangly0.github.io/tags/papper/>Papper</a></li></ul><nav class=paginav><a class=prev href=https://yangly0.github.io/posts/20250206094054/><span class=title>« Prev</span><br><span>速读论文M2BEV</span>
</a><a class=next href=https://yangly0.github.io/posts/20250120225031/><span class=title>Next »</span><br><span>设计模式之策略模式</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Yangly0/hugo-utterances issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>